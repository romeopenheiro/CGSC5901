---
title             : "CGSC5901: Advanced Statistics for Cognitive Science"
shorttitle        : "Assignment 1"
author: 
  - name          : "Romeo Penheiro"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "1125 Colonel By Dr, Ottawa, ON K1S 5B6"
    email         : "romeopenheiro@cmail.carleton.ca"
affiliation:
  - id            : "1"
    institution   : "Department of Cognitive Science, Carleton University"
authornote: |
  Graduate Student
  Masters in Cognitive Science
link-citations: true # make your citations hyperlinks to the corresponding bibliography entries.
floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no
documentclass     : "apa7"
classoption       : "man"
output            : papaja::apa6_pdf
editor_options: 
  markdown: 
    wrap: sentence
bibliography: references.bib
---

```{r packages, include=FALSE}
if (!requireNamespace('xfun')) install.packages('xfun')
xf <- loadNamespace('xfun')

cran_packages <- c(
        "remotes",
        "ggplot2",
        "dplyr",
        "ggpol",
        "kableExtra",
        "infer",
        "bibtex"
)

if (length(cran_packages) != 0) xf$pkg_load2(cran_packages)

gh_repos <- c(
        "crsh/papaja@devel", # Install the stable development versions from GitHub
        "romeopenheiro/assignment-1" # Import the data set from my GitHub
        
)
if (length(gh_repos) != 0) xf$install_github(gh_repos)

gg <- import::from(ggplot2, .all=TRUE, .into={new.env()})
dp <- import::from(dplyr, .all=TRUE, .into={new.env()})

import::from(magrittr, '%>%')
```

```{r setup}
# Set the decimal figures
options(scipen = 999, digits = 2)
# Set the chunk appearance to false
knitr::opts_chunk$set(echo = FALSE)
```

```{r message=FALSE}
# Create a data variable
clean_data <- assignment1::DATASET
```

```{r}
# Add a new column containing the percentage correct values
# Add a new column containing the group reaction time values
# Add a new column containing the log 10 of KF values
clean_data <-
        clean_data %>%
        dp$group_by(strings) %>%
        dp$mutate(group_percent_correct = mean(resp_corr, na.rm = TRUE)*100,
                  group_word_rt = mean(resp_rt, na.rm = TRUE),
                  log10_freq = log10(freq), .after = freq) %>%
        dp$ungroup()
```

# Introduction

The assignment is an analysis to assess, improve, and test the robustness of data that were collected from an experiment conducted on undergraduate students in the PSYC 2001 course.
The current experiment is a conceptual replication of Howes and Solomon's 1951 experiment [@howes1951].

Howes and Solomon investigated the visual duration threshold as a function of word-probability.
The study used Thorndike-Lorge's [-@thorndike1944] word count to measure word frequency because it was the largest corpus of words of that time.
The results showed a strong negative relationship between word frequency and the duration threshold that was necessary to correctly identify tachistoscopically-presented words.
In essence, words of low frequency (i.e., words that were used rarely) required longer presentation durations to be correctly identified compared to words of high frequency (i.e., words that were used often).
This suggested that word frequency is an important variable for models of word recognition.

The current experiment is different from the original experiment in two ways.
First, the experiment used a lexical decision task [@Meyer1971] instead of a threshold task that was used by Howes and Solomon [-@howes1951].
Second, the experiment was conducted using a computer in an online setting.
This design of experiment is preferable for two reasons.
First, to use Howes and Solomon's [-@howes1951] task in an online setting.
Second, to see whether a word frequency effect could be obtained using Howes and Solomon's stimuli [-@howes1951] in a different type of task.

The hypothesis of the experiment is to demonstrate similar word frequency as that observed in the Howes and Solomon's task [-@howes1951].

# Method

## Participant

```{r}
# Age descriptives
mean_age <- mean(clean_data$age, na.rm = T)
sd_age <- sd(clean_data$age, na.rm = T)
range_age <- range(clean_data$age, na.rm = T)
# Male/female/other? Age? Mean, SD, range of age.
```

```{r}
total_subject <-
        clean_data %>%
        dp$distinct(subject) %>%
        dp$count()
```

```{r}
# Count of gender
gender_count <-
        clean_data %>%
        dp$filter(trial_index == 0) %>%
        dp$count(gender)
```

The participants for the study were recruited through Carleton University's SONA system and each received a course credit in a psychology research methods course (PSYC 2001) for their participation.
`r total_subject` adults (*M* = `r mean_age`, *SD* = `r sd_age`, *range* = `r range_age[1]`-`r range_age[2]`, `r gender_count$n[1]` `r gender_count$gender[1]`, `r gender_count$n[2]` `r gender_count$gender[2]`, `r gender_count$n[3]` `r gender_count$gender[3]`) participated in the study.
One participant was excluded from the age and gender analysis due to incomplete data (`78`).

## Instrument

The experiment was created using [PsychoPy software](https://psychopy.org/index.html) [@peirce2019] and then converted to java script to run on [pavlovia](https://pavlovia.org).
Also, the design of the experiment restricted use to laptops or desktop computers due to screen size limitations of smaller devices.

## Stimuli

60 words of 6-12 letters in length were used as stimuli for the `word` condition.
Unlike Howes and Solomon's [-@howes1951] study that used the Thorndike-Lorge [-@thorndike1944] count values, the current study used word frequency values from Kucera and Francis [-@kuera1967], which is a more recent and widely-used word count.
In addition, 60 non-words were used in the `non-word` condition.
These were created by replacing the vowels in the words to create the non-words.

## Procedure

The experiment was a lexical decision task [@Meyer1971].
Each trial began with a fixation cross that was presented for one second.
When the fixation duration ended, a string of letters were presented for 100 ms. The task involved making a judgement on the string and pressing `w` on the keyboard if it was a word or pressing `n` on the keyboard if it was not a word.
The participants were asked to respond as quickly and as accurately as they could for each trial.
The next trial began as soon as a response was made.
If no response was made for a trial, the next trial would start automatically after ten seconds.
A total of 120 trials were presented with strings uniquely randomized for each participant.

## Data Cleaning

The raw data obtained from the study was cleaned in the following ways.
The practice trials were removed, and a subject number was added to each participant file.
The `gender` variable was renamed to `r gender_count$gender[1]`, `r gender_count$gender[2]`, `r gender_count$gender[3]`, or `r gender_count$gender[4]` (for missing values).
The missing values of reaction time for each participant was imputed by using median of the reaction time across the `word` or `non-word` trials for the respective participant.
The median value was used because it is less influenced by outliers compared to the mean.
The trials were no responses were provided were re-coded as `NA` for accuracy to differentiate incorrect trails from missed trials.
Although participant `78` had missing gender and age data, this participant was not excluded from the data analysis as the participant had complete responses.
The code for data cleaning and tidying can be obtained at the following [link](https://github.com/romeopenheiro/assignment-1/blob/main/data-raw/DATASET.R).

```{r}
# New dataframe containing unique words that will be used for viz
tidy_data <-
        clean_data %>%
        dp$filter(corrans == "w" & subject == 1) %>%
        dp$select(freq, log10_freq, group_percent_correct, group_word_rt)
        
```

(ref:acc-freq-caption) Scatterplot of the accuracy in percentage against Kucera and Francis word frequency for trials with correctly spelt words. The regression line is in blue with 95% confidence intervals.

```{r acc-freq, fig.cap = "(ref:acc-freq-caption)"}
gg$ggplot(data = tidy_data,
          mapping = gg$aes(x = freq,
                           y = group_percent_correct)) +
        gg$geom_point(alpha = 0.5,
                      size = 2) +
        gg$theme_minimal() +
        gg$labs(y = "Accuracy (%)",
                x = "Kucera & Francis \nword frequency ") +
        gg$geom_smooth(method = lm,
                       formula = 'y ~ x',
                       level = 0.95)

```

(ref:acc-log10-freq-caption) Scatterplot of the accuracy in percentage against log 10 of Kucera and Francis word frequency for trials with correctly spelt words. The regression line is in blue with 95% confidence intervals.

```{r acc-log10-freq, fig.cap = "(ref:acc-log10-freq-caption)"}
gg$ggplot(data = tidy_data,
          mapping = gg$aes(x = log10_freq,
                           y = group_percent_correct)) +
        gg$geom_point(alpha = 0.5,
                      size = 2) +
        gg$theme_minimal() +
        gg$labs(y = "Accuracy (%)",
                x = "log 10 of Kucera & Francis \nword frequency ") +
        gg$geom_smooth(method = lm,
                       formula = 'y ~ x',
                       level = 0.95)


```

(ref:group-word-rt-freq-caption) Scatterplot of the mean reaction time in seconds against Kucera and Francis word frequency for trials with correctly spelt words. The regression line is in blue with 95% confidence intervals.

```{r group-word-rt-freq, fig.cap = "(ref:group-word-rt-freq-caption)"}
gg$ggplot(data = tidy_data,
          mapping = gg$aes(x = freq,
                           y = group_word_rt)) +
        gg$geom_point(alpha = 0.5,
                      size = 2) +
        gg$theme_minimal() +
        gg$labs(y = "Mean reaction time (s)",
                x = "Kucera & Francis \nword frequency ") +
        gg$geom_smooth(method = lm,
                       formula = 'y ~ x',
                       level = 0.95)

```

(ref:group-word-rt-log10-freq-caption) Scatterplot of the the mean reaction time in seconds against log 10 of Kucera and Francis word frequency for trials with correctly spelt words. The regression line is in blue with 95% confidence intervals.

```{r group-word-rt-log10-freq, fig.cap = "(ref:group-word-rt-log10-freq-caption)"}
gg$ggplot(data = tidy_data,
          mapping = gg$aes(x = log10_freq,
                           y = group_word_rt)) +
        gg$geom_point(alpha = 0.5,
                      size = 2) +
        gg$theme_minimal() +
        gg$labs(y = "Mean reaction time (s)",
                x = "log 10 of Kucera & Francis \nword frequency ") +
        gg$geom_smooth(method = lm,
                       formula = 'y ~ x',
                       level = 0.95)
   
```

```{r}
# LDT accuracy (y) and KF word frequency (x)
a <- cor(tidy_data$freq, tidy_data$group_percent_correct, method = "pearson")
```

```{r}
# LDT accuracy (y) and log 10 of KF word frequency (x)
b <- cor(tidy_data$log10_freq, tidy_data$group_percent_correct, method = "pearson")
```

```{r}
# LDT RT (y) and KF word freq (x)
c <- cor(tidy_data$freq, tidy_data$group_word_rt, method = "pearson")
```

```{r}
# LDT RT (y) and log 10 of KF word frequency (x)
d <- cor(tidy_data$log10_freq, tidy_data$group_word_rt, method = "pearson")
```

(ref:overall-accuracy-caption) Table of the overall accuracy measure across words and non-words

```{r}
# Overall accuracy measure across words and non-words
overall_accuracy <-
        clean_data %>%
        dp$filter(subject == 1) %>%
        dp$group_by(corrans) %>%
        dp$mutate(corrans = dp$case_when(
                corrans == "w" ~ "Words",
                corrans == "n" ~ "Non-words"
        )) %>%
        dp$summarise(correct = mean(group_percent_correct)) %>%
        dp$arrange(desc(corrans))
```

```{r overall-accuracy}
kableExtra::kbl(x = overall_accuracy, 
                caption = "(ref:overall-accuracy-caption)",
                col.names = c("String type", "Mean overall Accuracy (%)")) %>%
        kableExtra::kable_classic(full_width = FALSE)
```

(ref:accuracy-words-caption) Table of the mean accuracy in percentage for each word string across participants

```{r}
# Mean accuracy for each word string across participants
mean_accuracy_words <- 
        clean_data %>%
        dp$filter(subject == 1, corrans == "w") %>%
        dp$arrange(strings) %>%
        dp$select(strings, group_percent_correct)
```

```{r accuracy-words}
kableExtra::kbl(x = mean_accuracy_words, 
                caption = "(ref:accuracy-words-caption)",
                col.names = c("Words", "Overall Accuracy (%)"),
                longtable = TRUE) %>%
        kableExtra::kable_classic(full_width = FALSE) 
        
        
```

(ref:accuracy-non-words-caption) Table of the mean accuracy in percentage for each non-word string across participants

```{r accuracy-non-words}
# Mean accuracy for each non-word string across participants
mean_accuracy_non_words <- 
        clean_data %>%
        dp$filter(subject == 1, corrans == "n") %>%
        dp$arrange(strings) %>%
        dp$select(strings, group_percent_correct)

kableExtra::kbl(x = mean_accuracy_non_words, 
                caption = "(ref:accuracy-non-words-caption)",
                col.names = c("Non-words", "Overall Accuracy (%)"),
                longtable = TRUE) %>%
        kableExtra::kable_classic(full_width = FALSE)

        
```

(ref:rt-words-caption) Table of the mean reaction time in seconds for each word string across participants

```{r rt-words}
# Mean reaction time for each word string across participants 
mean_rt_words <- 
        clean_data %>%
        dp$filter(subject == 1, corrans == "w") %>%
        dp$arrange(strings) %>%
        dp$select(strings, group_word_rt)

kableExtra::kbl(x = mean_rt_words, 
                caption = "(ref:rt-words-caption)",
                col.names = c("Words", "Mean reaction time (s)"),
                longtable = TRUE) %>%
        kableExtra::kable_classic(full_width = FALSE)

        
```

(ref:rt-non-words-caption) Table of the mean reaction time in seconds for each non-word string across participants

```{r rt-non-words}
# Mean reaction time for each non-word string across participants 
mean_rt_non_words <- 
        clean_data %>%
        dp$filter(subject == 1, corrans == "n") %>%
        dp$arrange(strings) %>%
        dp$select(strings, group_word_rt)

kableExtra::kbl(x = mean_rt_non_words, 
                caption = "(ref:rt-non-words-caption)",
                col.names = c("Non-words", "Mean reaction time (s)"),
                longtable = TRUE) %>%
        kableExtra::kable_classic(full_width = FALSE)

        
```

# Results

The results demonstrated two key findings.
First, words of low frequency were incorrectly identified more often when compared to high frequency words (see Figures \@ref(fig:acc-freq), \@ref(fig:acc-log10-freq)).
Second, words of low frequency (i.e., words that were used rarely) had longer reaction times to be correctly identified compared to high frequency words (i.e., words that were used often) (see Figures \@ref(fig:group-word-rt-freq), \@ref(fig:group-word-rt-log10-freq)).

The correlations of the associations were calculated for mean accuracy against word frequency (*`r a`*), and mean reaction time against word frequency (*`r c`*).
These associations revealed a medium positive strength of association and medium negative strength of association respectively, according to Cohen's d [@cohen2013].
In addition, the same associations were calculated but using log 10 transformed word frequency (*`r b`*, *`r d`* respectively).
These associations revealed a large positive strength of association and large negative strength of association respectively, according to Cohen's d [@cohen2013].

The overall accuracy measure across words and non-words was calculated to be `r overall_accuracy$correct[1]` and `r overall_accuracy$correct[2]` respectively (see Table \@ref(tab:overall-accuracy)).
Furthermore, descriptives were calculated for mean accuracy for each word string across participants (see Table \@ref(tab:accuracy-words)), mean accuracy for each non-word string across participants (see Table \@ref(tab:accuracy-non-words)), mean reaction time for each word string across participants (see Table \@ref(tab:rt-words)), and mean reaction time for each non-word string across participants (see Table \@ref(tab:rt-non-words)).

(ref:order-effects-caption) Scatterplot of the the mean reaction time in seconds against trial index for trials with words (left) and non-words (right). The regression line is in blue with 95% confidence intervals.

```{r order-effects, fig.cap = "(ref:order-effects-caption)"}
# Investigation of order effects
order_effects <-
        clean_data %>%
        dp$filter(subject == 1) %>%
        dp$select(trial_index, group_percent_correct, group_word_rt, corrans) %>%
        dp$mutate(corrans = dp$case_when(
                corrans == "w" ~ "Words",
                corrans == "n" ~ "Non-words"
        )) %>%
        dp$mutate(`String type` = corrans)

gg$ggplot(data = order_effects,
          mapping = gg$aes(x = trial_index,
                           y = group_word_rt,
                           fill = `String type`)) +
        gg$geom_point(alpha = 0.5,
                      size = 2) +
        gg$theme_minimal() +
        gg$labs(y = "Mean reaction time (s)",
                x = "Trial Index") +
        gg$geom_smooth(method = lm,
                       formula = 'y ~ x',
                       level = 0.95)
```

# Discussion

One confound that may have affected the outcome is order effects due to participant fatigue (see Figure \@ref(fig:order-effects)).
This confound may be eliminated or reduced in two ways: reducing the number of trials, and introducing a break in the middle of the experiment.

# References

::: {#refs}
:::
