results$pp$N.invalid.ngram      = sum(PP$Prop.Ngram > criteria.Ngram, na.rm = TRUE)
# Gender and age stats
results$pp$female    = PP %>% filter(gender=='Fe') %>% nrow()
results$pp$male      = PP %>% filter(gender=='Ma') %>% nrow()
results$pp$X         = PP %>% filter(gender=='X') %>% nrow()
results$pp$age.M     = round(mean(PP$age))
results$pp$age.SD    = round(sd(PP$age),1)
# Language stats (I know, this is lazy...)
results$pp$N.native    = round(PP %>% filter(nativeLanguage %in% nativeLanguages) %>% nrow() / results$pp$N * 100)
results$pp$N.america   = round(PP %>% filter(nativeLanguage == 'United States') %>% nrow() / results$pp$N * 100)
results$pp$N.canada    = round(PP %>% filter(nativeLanguage == 'Canada') %>% nrow() / results$pp$N * 100)
results$pp$N.uk        = round(PP %>% filter(nativeLanguage == 'United Kingdom') %>% nrow() / results$pp$N * 100)
results$pp$N.australia = round(PP %>% filter(nativeLanguage == 'Australia') %>% nrow() / results$pp$N * 100)
# Education
results$pp$N.education = PP %>% group_by(education) %>% summarise(Freq = n())
# Percentage removed
nPP         = PP %>% group_by(Status) %>% summarise(Freq = n())
results$pp$N.invalid = round(100 * nPP %>% filter(!Status=='Valid') %>% summarise(Freq = sum(Freq)))
results$pp$N.valid = PP %>% filter(Status=='Valid') %>% nrow()
# Remove from data
X           = X %>% filter(participantID %in% PP$participantID[PP$Status=='Valid'])
# Verify sufficient responses in the set of cues after removal of invalid pp's
Cues          = X %>% group_by(cue) %>% summarise(Freq = n())
missing       = Cues %>% filter(Freq < responseCountTreshold) %>% arrange(Freq)
missing$Freq  = (responseCountTreshold - missing$Freq)/3
# Sanity check
message('Number of participants missing: ', ceiling(sum(missing$Freq)/listlength.default))
# Convert to wide again
# Select 100 responses per row, by considering first: native American, Australian - Canadian,  - Irish - UK, others
# next by considering the date (most recent first), but always ordered by participants to include complete response sets at the participant level where possible
# Use top N
X_wide    = X %>%  select(id,participantID,age,gender,nativeLanguage,country,education,created_at,cue,response,RPOS) %>% spread(RPOS,response)
# Add a selection variable to favor native speakers
X_wide    = X_wide %>% mutate(Native = ifelse(nativeLanguage == 'United States', 3,
ifelse(nativeLanguage %in% c('Canada','Australia','New Zealand','Jamaica','Puerto Rico','South Africa'),2,
ifelse(nativeLanguage %in% c('Ireland','United Kingdom'), 1,0)))) %>%
arrange(participantID)
# Create a sample key to avoid ties in top_n, but sample weighted on native language
d = as.double(max(X_wide$participantID))
X_wide$SampleKey = X_wide$Native + (X_wide$participantID/d)
# Collect 100 participants depending on Native and date created (this could be improved)
X_set     = X_wide %>% group_by(cue) %>% top_n(100,SampleKey) %>% arrange(participantID) %>%
select(id,participantID,age,gender,nativeLanguage,country,education,created_at,cue,R1,R2,R3)
#message('Final number of responses: ', X_set %>% nrow())
results$responses$N.set100 =  X_set %>% nrow()
PP.set    = X_set %>% group_by(participantID,nativeLanguage,gender,age,education) %>%
summarise(N = n_distinct(participantID)) %>% nrow()
# Uncomment the following to check whether sufficient participants per cue are available
#PP.validset = nPP %>% filter(Status == 'Valid') %>% select(Freq) - PP.set
#message('Set 100 Participants removed: ',PP.validset, ' (',round(PP.validset / nPP %>% filter(Status == 'Valid') %>% select(Freq) * 100,2), '%)')
results$responses$N.valid = X_wide %>% nrow() * 3
results$responses$N.set100 = X_set %>% nrow() * 3
results$responses$N.valid - results$responses$N.set100
results$pp$N.set100  = X_set %>% group_by(participantID) %>% summarise(n_distinct(participantID)) %>% nrow()
# Write the dataset with 100 responses per cue
write.csv(X_set,output.file)
# Write a summary of the output to an rds file
saveRDS(results,report.file,ascii=TRUE)
# Clean up
rm(list = ls())
# Preprocessing pipeline for SWOW-EN project
#
# Note: A similar pipeline is implemented server side, which is used to determine
# the total number of valid responses per cue and determine the snowball sampling
# This pipeline reimplements some of its principles in a more systematic way.
#
# The script writes a fixed number of participants per cue (here set to 100)
# to an output file used for further processing
# To determine which data to retain, a number of checks are performed to see if participants
# are fluent speakers (knowing most cues, responding with words part of the language' lexicon), etc.
# In this script, we also prioritise native speakers if sufficient data is available for a specific cue.
#
# Author: Simon De Deyne, simon2d@gmail.com
# Last changed 13 June, 2019
library('stringr')
library('tidyverse')
# Settings
source('settings.R')
results      = list()
# Import the raw dataset
data.file    = './data/2018/raw/SWOW-EN.complete.csv'
output.file  = './data/2018/processed/SWOW-EN.R100.csv'
lexicon.file = './data/dictionaries/wordlist.txt'
cueCorrections.file = './data/dictionaries/cueCorrections.txt'
report.file  = './output/2018/reports/preprocessing.SWOW-EN.rds'
X            = read.table(data.file, header = TRUE, sep = ",", dec = ".", quote = "\"",stringsAsFactors = FALSE,
encoding = 'UTF-8')
Lexicon      = read.csv(lexicon.file, header = TRUE,stringsAsFactors = FALSE, encoding = 'UTF-8')
# If participants repeat the same response to a specific cue, recode as no more responses
doubles = X %>% filter(R1 == R2, R1 != unknown.Token) %>% select(participantID,cue,R1,R2) %>% nrow()
doubles = doubles + X %>% filter(R2 == R3, R2 != missing.Token,R2 != unknown.Token) %>%
select(participantID,cue,R1,R2)  %>% nrow()
X       = X %>% mutate(R2 = ifelse(R1 == R2 & R1 != unknown.Token, missing.Token,R2))
X       = X %>% mutate(R3 = ifelse(R2 == R3 & R2 != missing.Token & R2 != unknown.Token, missing.Token,R3))
# Swap inconsistent missing responses coded in R2 but not present in R3
inconsistent        = X$R2 ==missing.Token & X$R3 != missing.Token
X$R2[inconsistent]  = X$R3[inconsistent]
X$R3[inconsistent]  = missing.Token
results$responses$doubles = doubles
message('Removed ',doubles, ' repeated responses for a single cue')
# Convert data to long format
X           = gather(X,RPOS,response,R1,R2,R3,factor_key = FALSE)
# Original number of responses (unbalanced data)
results$responses$N.original = X %>% nrow()
message('Original number of responses: ', results$responses$N.original)
# Mark both unknown and missing responses. replace them by NA's for the remaining data
X           = X %>% mutate(isMissing = as.numeric(response == missing.Token))
X           = X %>% mutate(isUnknown = as.numeric(response  == unknown.Token))
X           = X %>% na_if(unknown.Token)
X           = X %>% na_if(missing.Token)
# Convert dates
X$created_at = as.POSIXct(strptime(X$created_at, format = "%Y-%m-%d %H:%M:%S",tz ='UTC'))
# Fix alternative spellings for cues
spelling.words  = read.table(cueCorrections.file,sep = '\t',header=TRUE,stringsAsFactors = FALSE, encoding = 'UTF-8')
X$cue           = plyr::mapvalues(X$cue,spelling.words$original,spelling.words$correction,warn_missing = FALSE)
# Count spaces and commas in responses, to figure out of n-grams with n > 1 are used. Ignore missing responses (Unknown word, No more responses")
X               = X %>% mutate(nWords = ifelse(isMissing >  0, NA, ifelse(str_count(response,"\\S+") + str_count(response,",|;") > 1,1,0)))
# Calculate presence of response in SUBTLEX  and VARCON wordlist (about 83% is present)
X               = X %>%  mutate(inLexicon = ifelse(isMissing > 0, NA, as.numeric(response %in% Lexicon$Word)))
# Calculate participant characteristics
PP              = X %>% group_by(participantID,nativeLanguage,gender,age,education) %>%
summarise(N = n(),
Unknown = sum(isUnknown),
Missing = sum(isMissing),
C.Response = n_distinct(response),
F.English = sum(inLexicon,na.rm = TRUE),
F.words = sum(nWords,na.rm = TRUE))
# Convert to proportions, note corrected for unknown and missing responses
PP$Prop.Unknown = PP$Unknown / PP$N
PP$Prop.Repeat  = 1 - (PP$C.Response - as.numeric(PP$Unknown>0) - as.numeric(PP$Missing>0)) / (PP$N - PP$Unknown - PP$Missing)
PP$Prop.X       = (PP$Missing + PP$Unknown) / PP$N
PP$Prop.English = PP$F.English / (PP$N - PP$Unknown - PP$Missing)
PP$Prop.Ngram   = PP$F.words / (PP$N - PP$Unknown - PP$Missing)
PP              = PP %>% mutate(Status = ifelse(Prop.X >  criteria.X, 'X',
ifelse(Prop.English < criteria.English,'Non-native',
ifelse(Prop.Repeat > criteria.Repeat, 'Perseveration',
ifelse(Prop.Ngram > criteria.Ngram, 'Verbose','Valid')))))
## Calculate the breakdown of valid and removed participants
results$pp$N                    = dim(PP)[1]
results$pp$N.invalid.X          = sum(PP$Prop.X > criteria.X)
results$pp$N.invalid.nonnative  = sum(PP$Prop.English < criteria.English, na.rm = TRUE)
results$pp$N.invalid.persever   = sum(PP$Prop.Repeat > criteria.Repeat, na.rm  = TRUE)
results$pp$N.invalid.ngram      = sum(PP$Prop.Ngram > criteria.Ngram, na.rm = TRUE)
# Gender and age stats
results$pp$female    = PP %>% filter(gender=='Fe') %>% nrow()
results$pp$male      = PP %>% filter(gender=='Ma') %>% nrow()
results$pp$X         = PP %>% filter(gender=='X') %>% nrow()
results$pp$age.M     = round(mean(PP$age))
results$pp$age.SD    = round(sd(PP$age),1)
# Language stats (I know, this is lazy...)
results$pp$N.native    = round(PP %>% filter(nativeLanguage %in% nativeLanguages) %>% nrow() / results$pp$N * 100)
results$pp$N.america   = round(PP %>% filter(nativeLanguage == 'United States') %>% nrow() / results$pp$N * 100)
results$pp$N.canada    = round(PP %>% filter(nativeLanguage == 'Canada') %>% nrow() / results$pp$N * 100)
results$pp$N.uk        = round(PP %>% filter(nativeLanguage == 'United Kingdom') %>% nrow() / results$pp$N * 100)
results$pp$N.australia = round(PP %>% filter(nativeLanguage == 'Australia') %>% nrow() / results$pp$N * 100)
# Education
results$pp$N.education = PP %>% group_by(education) %>% summarise(Freq = n())
# Percentage removed
nPP         = PP %>% group_by(Status) %>% summarise(Freq = n())
results$pp$N.invalid = round(100 * nPP %>% filter(!Status=='Valid') %>% summarise(Freq = sum(Freq)))
results$pp$N.valid = PP %>% filter(Status=='Valid') %>% nrow()
# Remove from data
X           = X %>% filter(participantID %in% PP$participantID[PP$Status=='Valid'])
# Verify sufficient responses in the set of cues after removal of invalid pp's
Cues          = X %>% group_by(cue) %>% summarise(Freq = n())
missing       = Cues %>% filter(Freq < responseCountTreshold) %>% arrange(Freq)
missing$Freq  = (responseCountTreshold - missing$Freq)/3
# Sanity check
message('Number of participants missing: ', ceiling(sum(missing$Freq)/listlength.default))
# Convert to wide again
# Select 100 responses per row, by considering first: native American, Australian - Canadian,  - Irish - UK, others
# next by considering the date (most recent first), but always ordered by participants to include complete response sets at the participant level where possible
# Use top N
X_wide    = X %>%  select(id,participantID,age,gender,nativeLanguage,country,education,created_at,cue,response,RPOS) %>% spread(RPOS,response)
# Add a selection variable to favor native speakers
X_wide    = X_wide %>% mutate(Native = ifelse(nativeLanguage == 'United States', 3,
ifelse(nativeLanguage %in% c('Canada','Australia','New Zealand','Jamaica','Puerto Rico','South Africa'),2,
ifelse(nativeLanguage %in% c('Ireland','United Kingdom'), 1,0)))) %>%
arrange(participantID)
# Create a sample key to avoid ties in top_n, but sample weighted on native language
d = as.double(max(X_wide$participantID))
X_wide$SampleKey = X_wide$Native + (X_wide$participantID/d)
# Collect 100 participants depending on Native and date created (this could be improved)
X_set     = X_wide %>% group_by(cue) %>% top_n(100,SampleKey) %>% arrange(participantID) %>%
select(id,participantID,age,gender,nativeLanguage,country,education,created_at,cue,R1,R2,R3)
#message('Final number of responses: ', X_set %>% nrow())
results$responses$N.set100 =  X_set %>% nrow()
PP.set    = X_set %>% group_by(participantID,nativeLanguage,gender,age,education) %>%
summarise(N = n_distinct(participantID)) %>% nrow()
# Uncomment the following to check whether sufficient participants per cue are available
#PP.validset = nPP %>% filter(Status == 'Valid') %>% select(Freq) - PP.set
#message('Set 100 Participants removed: ',PP.validset, ' (',round(PP.validset / nPP %>% filter(Status == 'Valid') %>% select(Freq) * 100,2), '%)')
results$responses$N.valid = X_wide %>% nrow() * 3
results$responses$N.set100 = X_set %>% nrow() * 3
results$responses$N.valid - results$responses$N.set100
results$pp$N.set100  = X_set %>% group_by(participantID) %>% summarise(n_distinct(participantID)) %>% nrow()
# Write the dataset with 100 responses per cue
write.csv(X_set,output.file)
# Write a summary of the output to an rds file
saveRDS(results,report.file,ascii=TRUE)
# Clean up
rm(list = ls())
library('tidyverse')
source('~/Documents/moving-research-online/word-cues/SWOWEN-2018-master/R/preprocessData.R')
# Settings
source('settings.R')
source('~/Documents/moving-research-online/word-cues/SWOWEN-2018-master/R/preprocessData.R', echo=TRUE)
source('~/Documents/moving-research-online/word-cues/SWOWEN-2018-master/R/preprocessData.R', echo=TRUE)
source('~/Documents/moving-research-online/word-cues/SWOWEN-2018-master/R/preprocessData.R', echo=TRUE)
source('~/Documents/moving-research-online/word-cues/SWOWEN-2018-master/R/preprocessData.R', echo=TRUE)
source('~/Documents/moving-research-online/word-cues/SWOWEN-2018-master/settings.R', echo=TRUE)
source('~/Documents/moving-research-online/word-cues/SWOWEN-2018-master/analysis.R', echo=TRUE)
source('~/Documents/moving-research-online/word-cues/SWOWEN-2018-master/R/createWordlist.R', echo=TRUE)
source('~/Documents/moving-research-online/word-cues/SWOWEN-2018-master/R/createWordlist.R', echo=TRUE)
source('~/Documents/moving-research-online/word-cues/SWOWEN-2018-master/R/createWordlist.R', echo=TRUE)
source('~/Documents/moving-research-online/word-cues/SWOWEN-2018-master/R/createWordlist.R', echo=TRUE)
source('~/Documents/moving-research-online/word-cues/SWOWEN-2018-master/analysis.R', echo=TRUE)
source('~/Documents/moving-research-online/word-cues/SWOWEN-2018-master/analysis.R', echo=TRUE)
source('~/Documents/moving-research-online/word-cues/SWOWEN-2018-master/R/createWordlist.R', echo=TRUE)
source('~/Documents/moving-research-online/word-cues/SWOWEN-2018-master/settings.R', echo=TRUE)
source('~/Documents/moving-research-online/word-cues/SWOWEN-2018-master/analysis.R', echo=TRUE)
source('~/Documents/moving-research-online/word-cues/SWOWEN-2018-master/R/createWordlist.R', echo=TRUE)
# Set the decimal figures
options(scipen = 999, digits = 2)
# Set the chunk appearance to false
knitr::opts_chunk$set(echo = FALSE)
if (!requireNamespace('xfun')) install.packages('xfun')
xf <- loadNamespace('xfun')
cran_packages <- c(
"remotes",
"ggplot2",
"dplyr",
"ggpol",
"ggpubr"
)
if (length(cran_packages) != 0) xf$pkg_load2(cran_packages)
gh_repos <- c(
"crsh/papaja", # Install the stable development versions from GitHub
"romeopenheiro/assignment-1" # Import the data set from my GitHub
)
if (length(gh_repos) != 0) xf$install_github(gh_repos)
gg <- import::from(ggplot2, .all=TRUE, .into={new.env()})
dp <- import::from(dplyr, .all=TRUE, .into={new.env()})
import::from(magrittr, '%>%')
tinytex::install_tinytex()
install.packages("tinytex")
tinytex::install_tinytex()
tinytex::install_tinytex()
install.packages("xfun")
install.packages("xfun")
tinytex::install_tinytex()
tinytex::install_tinytex()
install.packages("xfun")
install.packages("xfun")
install.packages("xfun")
install.packages("xfun")
install.packages("xfun")
install.packages("xfun")
tinytex::install_tinytex()
# Set the decimal figures
options(scipen = 999, digits = 2)
# Set the chunk appearance to false
knitr::opts_chunk$set(echo = FALSE)
if (!requireNamespace('xfun')) install.packages('xfun')
xf <- loadNamespace('xfun')
cran_packages <- c(
"remotes",
"ggplot2",
"dplyr",
"ggpol",
"ggpubr"
)
if (length(cran_packages) != 0) xf$pkg_load2(cran_packages)
gh_repos <- c(
"crsh/papaja@devel", # Install the stable development versions from GitHub
"romeopenheiro/assignment-1" # Import the data set from my GitHub
)
if (length(gh_repos) != 0) xf$install_github(gh_repos)
gg <- import::from(ggplot2, .all=TRUE, .into={new.env()})
dp <- import::from(dplyr, .all=TRUE, .into={new.env()})
import::from(magrittr, '%>%')
citation(rmarkdown)
citation("rmarkdown")
citation("tibble")
citation("gg")
citation("ggplot2")
if (!requireNamespace('xfun')) install.packages('xfun')
xf <- loadNamespace('xfun')
cran_packages <- c(
"remotes",
"ggplot2",
"dplyr",
"ggpol",
"kableExtra",
"infer"
)
if (length(cran_packages) != 0) xf$pkg_load2(cran_packages)
gh_repos <- c(
"crsh/papaja@devel", # Install the stable development versions from GitHub
"romeopenheiro/assignment-1" # Import the data set from my GitHub
)
if (length(gh_repos) != 0) xf$install_github(gh_repos)
gg <- import::from(ggplot2, .all=TRUE, .into={new.env()})
dp <- import::from(dplyr, .all=TRUE, .into={new.env()})
import::from(magrittr, '%>%')
# Set the decimal figures
options(scipen = 999, digits = 2)
# Set the chunk appearance to false
knitr::opts_chunk$set(echo = FALSE)
# Create a data variable
clean_data <- assignment1::DATASET
# Add a new column containing the percentage correct values
# Add a new column containing the group reaction time values
# Add a new column containing the log 10 of KF values
clean_data <-
clean_data %>%
dp$group_by(strings) %>%
dp$mutate(group_percent_correct = mean(resp_corr, na.rm = TRUE)*100,
group_word_rt = mean(resp_rt, na.rm = TRUE),
log10_freq = log10(freq), .after = freq) %>%
dp$ungroup()
# Age descriptives
mean_age <- mean(clean_data$age, na.rm = T)
sd_age <- sd(clean_data$age, na.rm = T)
range_age <- range(clean_data$age, na.rm = T)
# Male/female/other? Age? Mean, SD, range of age.
total_subject <-
clean_data %>%
dp$distinct(subject) %>%
dp$count()
# Count of gender
gender_count <-
clean_data %>%
dp$filter(trial_index == 0) %>%
dp$count(gender)
# New dataframe containing unique words that will be used for viz
tidy_data <-
clean_data %>%
dp$filter(corrans == "w" & subject == 1) %>%
dp$select(freq, log10_freq, group_percent_correct, group_word_rt)
gg$ggplot(data = tidy_data,
mapping = gg$aes(x = freq,
y = group_percent_correct)) +
gg$geom_point(alpha = 0.5,
size = 2) +
gg$theme_minimal() +
gg$labs(y = "Accuracy (%)",
x = "Kucera & Francis \nword frequency ") +
gg$geom_smooth(method = lm,
formula = 'y ~ x',
level = 0.95)
gg$ggplot(data = tidy_data,
mapping = gg$aes(x = log10_freq,
y = group_percent_correct)) +
gg$geom_point(alpha = 0.5,
size = 2) +
gg$theme_minimal() +
gg$labs(y = "Accuracy (%)",
x = "log 10 of Kucera & Francis \nword frequency ") +
gg$geom_smooth(method = lm,
formula = 'y ~ x',
level = 0.95)
gg$ggplot(data = tidy_data,
mapping = gg$aes(x = freq,
y = group_word_rt)) +
gg$geom_point(alpha = 0.5,
size = 2) +
gg$theme_minimal() +
gg$labs(y = "Mean reaction time (s)",
x = "Kucera & Francis \nword frequency ") +
gg$geom_smooth(method = lm,
formula = 'y ~ x',
level = 0.95)
gg$ggplot(data = tidy_data,
mapping = gg$aes(x = log10_freq,
y = group_word_rt)) +
gg$geom_point(alpha = 0.5,
size = 2) +
gg$theme_minimal() +
gg$labs(y = "Mean reaction time (s)",
x = "log 10 of Kucera & Francis \nword frequency ") +
gg$geom_smooth(method = lm,
formula = 'y ~ x',
level = 0.95)
# LDT accuracy (y) and KF word frequency (x)
a <- cor(tidy_data$freq, tidy_data$group_percent_correct, method = "pearson")
# LDT accuracy (y) and log 10 of KF word frequency (x)
b <- cor(tidy_data$log10_freq, tidy_data$group_percent_correct, method = "pearson")
# LDT RT (y) and KF word freq (x)
c <- cor(tidy_data$freq, tidy_data$group_word_rt, method = "pearson")
# LDT RT (y) and log 10 of KF word frequency (x)
d <- cor(tidy_data$log10_freq, tidy_data$group_word_rt, method = "pearson")
# Overall accuracy measure across words and non-words
overall_accuracy <-
clean_data %>%
dp$filter(subject == 1) %>%
dp$group_by(corrans) %>%
dp$mutate(corrans = dp$case_when(
corrans == "w" ~ "Words",
corrans == "n" ~ "Non-words"
)) %>%
dp$summarise(correct = mean(group_percent_correct)) %>%
dp$arrange(desc(corrans))
kableExtra::kbl(x = overall_accuracy,
caption = "(ref:overall-accuracy-caption)",
col.names = c("String type", "Mean overall Accuracy (%)")) %>%
kableExtra::kable_classic(full_width = FALSE)
# Mean accuracy for each word string across participants
mean_accuracy_words <-
clean_data %>%
dp$filter(subject == 1, corrans == "w") %>%
dp$arrange(strings) %>%
dp$select(strings, group_percent_correct)
kableExtra::kbl(x = mean_accuracy_words,
caption = "(ref:accuracy-words-caption)",
col.names = c("Words", "Overall Accuracy (%)"),
longtable = TRUE) %>%
kableExtra::kable_classic(full_width = FALSE)
# Mean accuracy for each non-word string across participants
mean_accuracy_non_words <-
clean_data %>%
dp$filter(subject == 1, corrans == "n") %>%
dp$arrange(strings) %>%
dp$select(strings, group_percent_correct)
kableExtra::kbl(x = mean_accuracy_non_words,
caption = "(ref:accuracy-non-words-caption)",
col.names = c("Non-words", "Overall Accuracy (%)"),
longtable = TRUE) %>%
kableExtra::kable_classic(full_width = FALSE)
# Mean reaction time for each word string across participants
mean_rt_words <-
clean_data %>%
dp$filter(subject == 1, corrans == "w") %>%
dp$arrange(strings) %>%
dp$select(strings, group_word_rt)
kableExtra::kbl(x = mean_rt_words,
caption = "(ref:rt-words-caption)",
col.names = c("Words", "Mean reaction time (s)"),
longtable = TRUE) %>%
kableExtra::kable_classic(full_width = FALSE)
# Mean reaction time for each non-word string across participants
mean_rt_non_words <-
clean_data %>%
dp$filter(subject == 1, corrans == "n") %>%
dp$arrange(strings) %>%
dp$select(strings, group_word_rt)
kableExtra::kbl(x = mean_rt_non_words,
caption = "(ref:rt-non-words-caption)",
col.names = c("Non-words", "Mean reaction time (s)"),
longtable = TRUE) %>%
kableExtra::kable_classic(full_width = FALSE)
# Bootstrap for mean reaction time vs log 10 of frequency
bootstrap_distribution <- tidy_data %>%
infer::specify(formula = group_word_rt ~ log10_freq) %>%
infer::hypothesize(null = "independence") %>%
infer::generate(reps = 1000, type = "permute") %>%
infer::calculate(stat = "correlation")
percentile_ci <- bootstrap_distribution %>%
infer::get_confidence_interval(level = 0.95, type = "percentile")
infer::visualize(bootstrap_distribution) +
infer::shade_confidence_interval(endpoints = percentile_ci) +
infer::shade_p_value(obs_stat = d, direction = "left") +
gg$theme_minimal()
# Bootstrap for group percentage correct vs log 10 of frequency
bootstrap_distribution <- tidy_data %>%
infer::specify(formula = group_percent_correct ~ log10_freq) %>%
infer::hypothesize(null = "independence") %>%
infer::generate(reps = 1000, type = "permute") %>%
infer::calculate(stat = "correlation")
percentile_ci <- bootstrap_distribution %>%
infer::get_confidence_interval(level = 0.95, type = "percentile")
infer::visualize(bootstrap_distribution) +
infer::shade_confidence_interval(endpoints = percentile_ci) +
infer::shade_p_value(obs_stat = b, direction = "right") +
gg$theme_minimal()
# Investigation of order effects
order_effects <-
clean_data %>%
dp$filter(subject == 1) %>%
dp$select(trial_index, group_percent_correct, group_word_rt, corrans) %>%
dp$mutate(corrans = dp$case_when(
corrans == "w" ~ "Words",
corrans == "n" ~ "Non-words"
)) %>%
dp$mutate(`String type` = corrans)
gg$ggplot(data = order_effects,
mapping = gg$aes(x = trial_index,
y = group_word_rt,
fill = `String type`)) +
gg$geom_point(alpha = 0.5,
size = 2) +
gg$theme_minimal() +
gg$labs(y = "Mean reaction time (s)",
x = "Trial Index") +
gg$geom_smooth(method = lm,
formula = 'y ~ x',
level = 0.95)
# to cite specific packages
knitr::write_bib("cran_packages", "references.bib")
my_packages <- as.data.frame(installed.packages()[ , c(1, 3:4)])            # Apply installed.packages()
my_packages <- my_packages[is.na(my_packages$Priority), 1:2, drop = FALSE]  # Keep NA rows
rownames(my_packages) <- NULL                                               # Rename rows
View(my_packages)
citation("xfun")
xfun, usethis, tinytex, tidyverse, systemfonts, tibble, roxygen2, rmarkdown, remotes, readr, papaja, kableExtra, knitr, ggplot2, dplyr, data.table
, assignment1
# to cite specific packages
cite_packages <- c("xfun", "usethis", "tinytex", "tidyverse", "systemfonts", "tibble", "roxygen2", "rmarkdown", "remotes", "readr", "papaja", "kableExtra", "knitr", "ggplot2", "dplyr", "data.table", "assignment1")
knitr::write_bib("cite_packages", file = "references.bib")
source("~/.active-rstudio-document", echo=TRUE)
getwd()
setwd("/Users/RomeoPenheiro_1/Desktop/PENHEIRO_ROMEO/")
